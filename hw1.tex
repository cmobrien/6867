\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
%\pagenumbering{arabic}


% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
%\usepackage[pdftex]{hyperref}
%\hypersetup{
%pdftitle={L@S 2014 Work-in-Progress Format},
%pdfauthor={LaTeX},
%pdfkeywords={SIGCHI, proceedings, archival format},
%bookmarksnumbered,
%pdfstartview={FitH},
%colorlinks,
%citecolor=black,
%filecolor=black,
%linkcolor=black,
%urlcolor=black,
%breaklinks=true,
%}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}

\begin{document}

\title{Homework 1}


\maketitle

\section{Gradient Descent}


\subsection{Part 2}

Choosing a starting guess that is close to a solution will make the function converge more quickly. Choosing a starting guess that's on a downward slope into a particular solution will probably make the ultimate result of gradient descent be that solution; therefore, you'll get a better solution if the starting guess is on a downward slope into a global maximum, and a worse solution if the starting guess is on a downward slope into a mediocre local maximum.

A lower step size will make the function yield better results but also converge more slowly.

A lower convergence criterion will make the function yield better results but also converge more slowly. Also, a low convergence criterion combine with a high step size could create the risk that the process will never terminate.

\subsection{Part 3}

See attached code.

\subsection{Part 4}

Our gradient descent procedure, with a step size of 0.1 and convergence criterion of 0.1, took 360 function calls to converge to the correct solution in the quadratic bowl, whereas the scipy optimizer fmin\_bfgs took 16 function calls. When we increased the step size and convergence criterion to 1, our procedure instead took 30 function calls; however, this number is somewhat misleading, because had we chosen the initial guess (or the solution!) to not be integers, our algorithm would have converged to a substantially inferior result. Moreover, as we made the initial guess be farther and farther from the true solution, our optimizer took longer and longer to converge, whereas the scipy optimizer did not use more function calls. It would seem, therefore, that the scipy optimizer is much faster and more effective than our optimizer, particularly for guesses distant from the solution.

\section{Linear Basis Function Regression}

\subsection{Part 1}

See attached code.

\subsection{Part 2}

See attached code.

\subsection{Part 3}

% NEEDS WRITING

\section{Ridge Regression}

\subsection{Part 1}

% NEEDS WRITING

\subsection{Part 2}

% NEEDS WRITING

\subsection{Part 3}

% NEEDS WRITING

\section{Generalizations}

\subsection{Part 1}

% NEEDS WRITING

\subsection{Part 2}

% NEEDS WRITING

\subsection{Part 3}

The difference in approaches in the previous two questions (in other words, punishing error linearly versus punishing error quadratically) is reflected in the different way the two approaches handle outliers. If you expected your data to have a lot of outliers that you don't think accurately reflect  the future correct answers, you should probably use absolute deviations. If your data is generally pretty clumped, with few misleading outliers, you probably want squared deviations, because otherwise you run the risk that some of your data points might not get as much weight as they deserve. 

You might want to use the lasso when you have a lot of features and you want to ignore some of them entirely. Linear punishment will push many of the weights all the way to 0, and then your function will be simple to explain, implement, and think about intuitively.

If your prediction problem has absolute value loss, it becomes likelier that you'll want to use the method from 4-1, which seeks to minimize the absolute differences between prediction and result rather than the squared difference between prediction and result. This is because presumably you want to train your predictor in such a way that it minimizes the loss function that you will suffer when you use your predictor on real data. How much you punish high weight vectors, on the other hand, is more about how scared you are of overfitting than it is about the details of your loss function; an unusual loss function is unlikely to make you change how much and in what way you punish high weights.

\end{document}