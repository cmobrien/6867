\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
%\pagenumbering{arabic}



\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
%\usepackage[pdftex]{hyperref}
%\hypersetup{
%pdftitle={L@S 2014 Work-in-Progress Format},
%pdfauthor={LaTeX},
%pdfkeywords={SIGCHI, proceedings, archival format},
%bookmarksnumbered,
%pdfstartview={FitH},
%colorlinks,
%citecolor=black,
%filecolor=black,
%linkcolor=black,
%urlcolor=black,
%breaklinks=true,
%}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}

\begin{document}

\title{Exploring Classification}


\maketitle

\large

In this paper, we will discuss two approaches for classifying data: support vector machines and logistic regression. We will begin by with the binary classification problem, and then we will generalize our techniques to the solve multi-classification.


\section{Support Vector Machine}


We implemented the dual form of linear SVM's with slack variables in Python. To do so, we converted the input into a quadratic program, which we used \texttt{cvxopt.solvers.qp} to solve. Recall the general form of the quadratic program for the dual form of the SVM, which is given by

\normalsize
\begin{center}\begin{tabular}{r p{0.0in} l }
maximize && $ \displaystyle \sum_{i =1}^n \alpha_i - \frac{1}{2}\sum_{i = 1}^{n}\sum_{j = 1}^n \alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)}\cdot x^{(j)}) $
\\ && \\
subject to && $ \displaystyle 0 \leq \alpha_i \leq C$ 
\\ && \\
&&		      $ \displaystyle \sum_{i = 1}^{n} \alpha_iy^{(i)} = 0$
\end{tabular}\end{center}

\large

We will demonstrate the formulation of this program in the matrix form required for \texttt{cvxopt.solvers.qp} on a simple example. Consider the dataset with positive examples $(1, 2)$ and $(2, 2)$ and negative examples $(0, 0)$ and $(-2, 3)$.
If we order the dataset according to the order presented here, then in matrix notation, the quadratic program above is given by

\normalsize
\begin{center}\begin{tabular}{r p{0.0in} l }
minimize &&\\
&&
$ \displaystyle \frac{1}{2}\alpha^T
\begin{bmatrix}
5 & 6 & 0 & -4\\
6 & 8 & 0 & -2\\
0 & 0 & 0 & 0\\
-4 & -2 & 0 & 13\\
\end{bmatrix}
\alpha +
\begin{bmatrix}
-1\\ -1\\ -1\\ -1\\
\end{bmatrix}^T
\alpha
 $
\\ && \\
subject to &&\\
&& $
\begin{bmatrix}
1 & 0 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & -1\\
\end{bmatrix}
\alpha
\leq
\begin{bmatrix}
C\\
0\\
C\\
0\\
C\\
0\\
C\\
0\\
\end{bmatrix}
$ 
\\ && \\
&&		      $ 
\begin{bmatrix}
1& 1& -1& -1\\
\end{bmatrix}
\alpha = \begin{bmatrix}0\end{bmatrix}
$
\end{tabular}\end{center}

\large
Solving this linear program for $\alpha$, we obtain the linear separator shown in Figure 1. For this small example, we can clearly see that this separator maximizes the margin.

\begin{figure}
\centering
\includegraphics[width=2.25in]{plots/1-3/casey/simple.png}
\caption{The separator and margin computed by our SVM implementation using a small example.}
\end{figure}

\subsection{Applications}

Next, we try out our implementation on four different data sets, each consisting of 400 points for each of the training, validation, and test sets. The first three data sets, which we will refer to as \texttt{stdev1}, \texttt{stdev2}, \texttt{stdev4}, consist of points where each of the negative and positive examples are generated from a Gaussian distribution with varying standard deviations (as suggested by the name). The final data set, \texttt{nonSep}, is a data set generated as to be impossible to separate with a linear separator.

\begin{figure*}[!ht]
\centering
\begin{tabular}{c c c}
\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev1c1train.png}
	\caption{\texttt{stdev1} Training Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev2c1train.png}
	\caption{\texttt{stdev2} Training Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev4c1train.png}
	\caption{\texttt{stdev4} Training Data}
\end{subfigure} \\

\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev1c1val.png}
	\caption{\texttt{stdev1} Validation Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev2c1val.png}
	\caption{\texttt{stdev2} Validation Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/1-3/casey/stdev4c1val.png}
	\caption{\texttt{stdev4} Validation Data}
\end{subfigure} \\
\end{tabular}
\caption{The separators computed by our SVM implementation on a variety of different data sets with $C = 1$.}
\end{figure*}

For each of these data sets we will use a value of $C = 1$, and we will show the resulting boundary on both a training and validation set. We will also report the training and validation error rates. We begin with \texttt{stdev1}.

Figures 2(a) and 2(b) show the decision boundary formed by our algorithm on the training and validation data, respectively. As we can clearly see, both the training and validation data have an error rate of 0. This is not surprising given the very large gap between positive and negative samples.

Figure 2 also shows the results of repeating the process with \texttt{stdev2} and \texttt{stdev4}. For \texttt{stdev2}, the training error rate was 9.25\% and the validation error rate was 8.25\%. For \texttt{stdev4}, the training error rate was 26.5\% and validation error rate 23.5\%.

We can clearly see from the plots that \texttt{stdev2} and \texttt{stdev4} are not linearly separable (both in training and validation data), and thus we are not surprised by the increased error rates. It is interesting to note that the training data has higher error rates than the validation data in both cases. However, since both training data and validation data were drawn from Gaussian distributions, this is not terribly unlikely and does not surprise us.


Next, we focus on \texttt{nonSep}. The resulting decision boundaries are shown in Figure 3. Clearly, this data is ill-fitted for classification via a linear separator.

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/nonSepc1train.png}
	\caption{\texttt{nonSep} Training Data}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/nonSepc1val.png}
	\caption{\texttt{nonSep} Validation Data}
\end{subfigure}
\caption{The resulting linear separator when the data is not linearly separable.}
\end{figure}

\begin{figure*}
\centering
\renewcommand*{\arraystretch}{1.5}

\begin{tabular}{c c}
\begin{subfigure}[b]{3.5in}
\centering
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	& \texttt{stdev1} & \texttt{stdev2} & \texttt{stdev4} & \texttt{nonSep}\\
	\hline
	$C$ & 1 & 0.01 & 0.01 & 10 \\
	\hline
	$\beta$ & 0.1 & 0.1 & 0.01 & 1 \\
	\hline
	SVs & 22 & 400 & 100 & 99 \\
	\hline
	Test Error & 0.25\% & 6\% & 21.75\% & 5\% \\
	\hline
	\end{tabular}
	\caption{Gaussian Kernel}
\end{subfigure}
&
\begin{subfigure}[b]{3.5in}
\centering
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	& \texttt{stdev1} & \texttt{stdev2} & \texttt{stdev4} & \texttt{nonSep}\\
	\hline
	$C$ & 10 & 0.01 & 0.01 & 1 \\
	\hline
	SVs & 3 & 125 & 249 & 392 \\
	\hline
	Test Error & 0 & 7.25\% & 22\% & 49.5\%\\
	\hline
	\end{tabular}
	\caption{Linear Kernel}
\end{subfigure}
\end{tabular}

\caption{The parameters which minimized validation error rates for both the linear kernel and the Gaussian kernel on each data set. The number of support vectors resulting from these parameters and the test error rate are also shown.}
\end{figure*}

With a training error of 48.5\%, we can see that the separator is barely able to do a better job than randomly guessing how to classify each point. On the validation data, we see an error rate of 50.75\%. Because we have an error rate of over 50\%, we could have done a better job if we guessed the exact opposite of what our trained separator suggested. If we were not sure before, then the very high error rates convince us that a linear separator is not appropriate for this data.

\subsection{Kernels}

Next, we extended our dual form SVM to operate with kernels. We experimented on our data using the Gaussian kernel, with a variety of values for $C$ and $\beta$. For each data set, we found the values of $C$ and $\beta$ which minimized the validation error rate (see the next section for a discussion of this decision). We repeated this experiment for the linear kernel, again using the validation error rate to tune $C$. The results are summarized in Figure 4.


To get an idea for how the classifier works using a Gaussian kernel, Figure 5 shows the resulting classifier for \texttt{stdev1} and \texttt{nonSep}. We can clearly see that the classifier is no longer linear. In addition, we can see that this allows for it to handle the \texttt{nonSep} data in a way which the linear separator could not.

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/nonSepguass2.png}
	\caption{\texttt{stdev1} Training Data}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/nonSepgauss.png}
	\caption{\texttt{nonSep} Training Data}
\end{subfigure}
\caption{The resulting separators when using the Gaussian kernel (with the optimal parameters shown in Figure 4).}
\end{figure}

For \texttt{stdev1}, \texttt{stdev2}, and \texttt{stdev4}, we can see that the test error rates resulting from both the Gaussian and linear kernels are very similar. From looking at the data sets, it seems that a linear separator is the most appropriate choice for the data. Thus, even when we apply a Gaussian separator, which allows a lot more flexibility, we still find that the linear separator performs roughly equally well.

For \texttt{nonSep}, we see a much different story. The data is clearly not linearly separable. Thus, we are hardly able to achieve an error rate below 50\% with the linear separator. However, when we apply the Gaussian separator, we are able to achieve an error rate of only 5\%. Thus, the added flexibility of the Gaussian kernel allows us to use the same techniques to separate data which is not linearly separable.

Figure 4 also shows the number of support vectors that the optimal parameters returned. We note that these numbers may be slightly misleading. Because \texttt{cvxopt.solvers.qp} never actually drove $\alpha$ values completely to zero, we had to choose a threshold such that we considered only $\alpha$ values above the threshold to be support vectors. For all of our experiments, we chose a value of $10^{-5}$. From observation, this value seemed to most accurately represent a cutoff between $\alpha$ values which were significant and those which seemed as if they should be considered zero.

It is interesting to note that for all data sets other than \texttt{stdev1} (which was completely linearly separable and thus did not actually require any slack), the number of support vectors in the optimal solution was very high. It is possible that our threshold for deciding support vectors just did not do an adequate job of deciding which points should be considered support vectors. However, experimenting with the threshold value and the parameters of the solver did not yield any better results. We also note that for \texttt{stdev2} and \texttt{stdev4}, the value of $C$ is very small. In the next section, we will discuss the effect that the value of $C$ has on the number of support vectors.

\subsection{Analyzing C}

Next, we performed experiments to analyze the effects of varying $C$. The purpose was to examine how the value of $C$ affects the value of the geometric margin and the number of support vectors chosen. Because the notion of geometric margin does not make sense for the Gaussian kernel, we performed these experiments using the linear kernel. However, we note that the dependence of the number of support vectors on $C$ was similar in the Gaussian kernel as in the linear kernel. We will show our results for data sets \texttt{stdev1} and \texttt{stdev2}. The results for the other two data sets are similar, so we do not display them here.

For each of \texttt{stdev1} and \texttt{stdev2}, we used $C$ in the set $\{0.01, 0.1, 1, 10, 100\}$, and computed the geometric margin and total number of support vectors. As a general trend, both the geometric margin and the number of support vectors decreased as we increased $C$. % Increasing C means less slack

We can see this in Figure 6, which shows the separator defined by $C = 0.01$ and $C = 100$ for \texttt{stdev1}. We can clearly see that the increased value of $C$ results in a smaller margin, and consequently much fewer vectors are support vectors.

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/c01.png}
	\caption{$C = 0.01$}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/casey/c100.png}
	\caption{$C = 100$}
\end{subfigure}
\caption{The effects of $C$ on the separator for \texttt{stdev1}, shown on the training data. Support vectors are shown as dots, all other points as plusses. We can see that the smaller $C$ leads to a much wider margin and more support vectors.}
\end{figure}

Figure 7 shows geometric margin and number of support vectors plotted as a function of $C$. Note that the $x$ axis is shown on a log scale for convenience. We can see that both the geometric margin and number of support vectors seem to decrease to a point and then level off. This is because as we increase $C$, we decrease the amount of slack that we allow. As $C$ approaches infinity, we approach the situation where we do not allow any slack, and the behavior levels off.

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/margin.png}
	\caption{Geometric Margin}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/1-3/support_vectors.png}
	\caption{Number of Support Vectors}
\end{subfigure}
\caption{TEST}
\end{figure}

We know that a support vector machine aims to maximize the size of the margin. Thus, a seemingly reasonable idea would be to use $C$ to maximize the value of the margin on the training data. However, from the trends we see above, this seems that it would just result in the choice of an arbitrarily small value for $C$, which we would have no reason to expect to perform well on the validation or test data.

A more reasonable choice is to choose $C$ based on the validation data. An option would be to maximize the size of the margin on the validation data. However, from Figure 6 we can see that increasing the margin can just lead to more vectors being classified as support vectors, which does not actually improve our classification accuracy. The option which tended to give better overall performance was to choose $C$ to minimize the validation error rate.


\section{Logistic Regression}

\subsection{Theory}

The kernelized form of the logistic regression objective can be derived as follows. We know that the negative log likelihood function, which we want to mininize, can be expressed as follows:

\begin{equation}NLL(w) = \sum_{i=0}^N \log(1+e^{-y^{(i)}(x^{(i)}\cdot w + w_0)})\end{equation}

where for a data point $i$, $y^{(i)}$ represents what class it belongs to and $x^{(i)}$ represents its feature vector. Together $w$ and $w_0$ together make up the weights, supplied by the algorithm and used to classify the points.

The algorithm will tend to classify a point as positive if $x^{(i)} \cdot w + w_0 > 0$ and as negative if $x^{(i)} \cdot w + w_0 < 0$. More precisely, the algorithm assigns a probability of $1/(1+e^{-W^T x})$ to the point's correct classification being positive, and a probability of $1 - (1/(1+e^{-W^T x}))$ to the point's correct classification being negative. In light of this, the above formula for a posteriori negative log likelihood makes sense; it is therefore our goal as algorithmicists to minimize this quantity as efficiently as possible.

The natural method to use to minimize the negative log likelihood is by the method of gradient descent. This will require repeated calls to the formula defined above, for an indefinite period of time before the value converges or our algorithm gives up.

Each such call takes a certain amount of time: given that the outer sum loops over $N$ different elements (where $N$ is the number of data points), and each call to the function requires us to evaluate $x^{(i)} \cdot w$, the dot product of two $d$-dimensional vectors (where $d$ is the dimensionality of the feature vectors), every call to $NLL(w)$ as defined above will take $\Theta(Nd)$ time to compute, assuming basic arithmetic operations are constant-time.

%This is efficient assuming $N = \Omega(d)$. However, in the case where $d >> N$, it is possible to do much better than this by making a clever assumption. We may assume that $W = X^T \alpha$. Here, $W$ is our $(d+1)$-dimensional vector of weights that helps us determine how we will ultimately classify points ($W$ includes $w$ and $w_0$, $X$ is the $N \times (d+1)$ data matrix). Finally, $\alpha$ is some new $N$-dimensional vector for us to consider, which roughly corresponds to how important each of the data points is to our classification. The core idea is to transform the above formula from being a formula over inputs $W$ (a $d$-dimensional vector) to a formula over inputs $\alpha$ (an $N$-dimensional vector). 

As we have seen from the \texttt{nonSep} data above, there are situations in which it is useful to apply an arbitrary kernel. In order to be able to do this, we make the substitution $W = X^T \alpha$. Here, $W$ is our $(d+1)$-dimensional vector of weights that helps us determine how we will ultimately classify points ($W$ includes $w$ and $w_0$, $X$ is the $N \times (d+1)$ data matrix). Finally, $\alpha$ is some new $N$-dimensional vector for us to consider, which roughly corresponds to how important each of the data points is to our classification. The core idea is to transform the above formula from being a formula over inputs $W$ (a $d$-dimensional vector) to a formula over inputs $\alpha$ (an $N$-dimensional vector). 

First, we rewrite equation (1) as
$$NLL(w) = \sum_{i=0}^N \log(1+e^{-y{(i)}(XW)^{(i)}})$$
where $(XW)^{(i)}$ denotes the $i^{\textrm{th}}$ element of the vector $XW$. Next, we make use of the assumption $W = X^T\alpha$. So then we have

\begin{center}
\begin{tabular}{r c l}
$NLL(\alpha)$&$=$&$\displaystyle \sum_{i=0}^N \log(1+e^{-y^{(i)}(XX^T\alpha)^{(i)}})$\\
&&\\
&$=$&$\displaystyle \sum_{i=0}^N \log(1+e^{-y{(i)}((XX^T)^{(i)} \cdot \alpha})$
\end{tabular}
\end{center}

We now have a new formula, with a new vector on which we can do gradient descent. Moreover, the dimensionality of the new vector is different from that of the old vector; $\alpha$ is an $N$-dimensional vector, in contrast to $W$, which was a $d$-dimensional vector. 

How long does it take to compute the formula above? %Well, naively, one might expect that we need to iterate over the sum $N$ times, and for each iteration of the sum, we need to compute the matrix product $XX^T$, which takes $\Theta(N^2d)$ time (the runtime of multiplying an $N \times d$ matrix with a $d \times N$ matrix). Then, we need to compute the dot product of $(XX^T)^{(i)}$, an $N$-dimensional vector, with $\alpha$, another $N$-dimensional vector, which would take $\Theta(N)$ time. Thus, at first glance, it appears that our new formula takes $\Theta(N^3 d)$ time to compute; not an improvement! 
First we note that the matrix $XX^T$ can be precomputed. Because it is independent of $\alpha$, there's no reason for us to compute it at every step of the gradient descent. With that consideration taken into account, each step of the gradient descent will now take $\Theta(N^2)$ time. We call this new form of the equation above the \emph{kernelized} form of the equation.% It allows us to efficiently find a weight vector $W$ even when the feature space is very large, by doing gradient descent on the $\alpha$ vectors, and then computing $W$ at the end by using the fact that $X^T\alpha = W$. 

%Moreover, this method can be used to substitute in an arbitrary kernel matrix in place of $XX^T$. Thus, this general method supports the use of any kernel one chooses, simplying by plugging in a precomputed kernel matrix and running gradient descent. 

One potential issue with this method is that it will have a tendency to assign a non-zero value to every $\alpha$, since in logistic regression, every point has some effect on the location of the classifier, albeit often small. This is not ideal; for many applications, it is convenient to have an $\alpha$ that is very sparse, in order to make computations involving $\alpha$ cheaper and in order to make the situation easier to describe and understand. 

For this reason, we introduced L1 regularization on the $\alpha$ values; in other words, we introduced an additional penalty of $\lambda ||\alpha||_1$ on the alpha vectors, in addition to the penalty corresponding to the negative log likelihood. Our overall penalty $P(\alpha)$ for a given vector $\alpha$ was: 

\begin{center}
\begin{tabular}{r c l}
$P(\alpha)$&$=$&$\displaystyle NLL(\alpha) + L1(\alpha)$\\
&&\\
&$=$&$\displaystyle \sum_{i=0}^N \log(1+e^{-y{(i)}((XX^T)^{(i)} \cdot \alpha}) + \lambda||\alpha||_1$\\
\end{tabular}
\end{center}

In order to introduce the L1 regularization without adversely affecting the performance of the gradient descent function (functions that are not smooth can disrupt gradient descent functions) we used the approximately-correct function 

$$L1(\alpha) \approx \lambda \sum_i \sqrt{||\alpha_i||^2 + \epsilon}$$

for some sufficiently small $\epsilon << \lambda$. 

\subsection{Experiment}

As a test of our methods, we used them on the four 2-dimensional data sets from the previous section. In these tests, we used the \texttt{fmin\_bfgs} function for our gradient descent, available online. Our methods yielded results that seemed to strongly imply an underlying mechanism that made sense. Our results, with $\lambda = 0$ (i.e., no L1 regalarization), are shown in Figure 8. It is instructive to see what happens when L1 regularization is introduced.

\begin{figure*}[!ht]
\centering
\begin{tabular}{c c c}
\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev1_train_plot.png}
	\caption{\texttt{stdev1} Training Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_train_plot.png}
	\caption{\texttt{stdev2} Training Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev4_train_plot.png}
	\caption{\texttt{stdev4} Training Data}
\end{subfigure} \\

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev1_test_plot.png}
	\caption{\texttt{stdev1} Test Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_test_plot.png}
	\caption{\texttt{stdev2} Test Data}
\end{subfigure} &

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev4_test_plot.png}
	\caption{\texttt{stdev4} Test Data}
\end{subfigure} \\
\end{tabular}
\caption{The separators computed by our logistic regression implementation on a variety of different data sets with $\lambda = 0$.}
\end{figure*}

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/nonSep2_train_plot.png}
	\caption{\texttt{nonSep} Training Data}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/nonSep2_test_plot.png}
	\caption{\texttt{nonSep} Test Data}
\end{subfigure}
\caption{The resulting separator on \texttt{nonSep}, with $\lambda = 0$.}
\end{figure}


\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_test_plot_lambda_01.png}
	\caption{$\lambda = 0.01$}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_test_plot_lambda_1.png}
	\caption{$\lambda = 1$}
\end{subfigure}
\caption{The resulting separator on \texttt{stdev2} Training Data.}

\end{figure}


In Figure 10, we see the results of using $\lambda = 0.01$ and $\lambda = 1$ on the \texttt{stdev2} training data. Notice that the result is barely changed at all by the introduction of L1 regularization. One difference we found very noticeable, however, was the great cost in additional time that occurred when we introduced L1 regularization; with the relatively low $\lambda$ value of 0.01, the gradient descent went from taking 26 seconds to terminate to 84 seconds, and with a $\lambda$ value of 1, the gradient descent took 153 seconds! This is probably due to the adverse effects of the L1 regularization term on the smoothness of the function. 

In addition to causing little effect on the shape and location of the separating line, we observed little difference in the $\alpha$ vector as a result of the introduction of an L1 regularization term. Final $\alpha$ values fluctuated slightly, usually downward, but no terms were driven to 0. Because of the great additional costs in computation time to introducing an L1 regularization term, and the lack of observed benefits, we decided to omit a regularization term from our experiments in the multi-class case. 

Regardless of how large we made $\lambda$, it remained true that all data points had nonzero values for $\alpha$. This implies that, in a strict sense, all points in every graph were ``support vectors''; they all contributed some nonzero amount to the location of the linear separator.

In the SVM case, we were able to determine a threshold which clearly divided points which were supposed to be support vectors and those which should have $\alpha$ values considered zero. For logistic regression, the $\alpha$ values were mostly around $10^{-3}$ or $10^{-4}$, with the largest magnitudes being no larger than 0.005. Therefore, rather than declare all points support vectors, which would be uninteresting, we decided to artifically create a threshold that would exclude about half the points; the threshold we decided upon was $\alpha = 0.001$. Points with $\alpha > 0.001$ we call ``support vectors,'' and other points we do not.

When we set $\lambda = 1$, the $\alpha$ values displayed an interesting pattern; the ones far from the separator tended to be support vectors, whereas the points close to the separator tended to not be. This is quite different from what occurred in the SVM case, where the opposite is true. We hypothesize that why we see this pattern in logistic regression is because the farthest points tend to have the most extreme probabilities (they have very small probabilities of coming from either class) and as a result, their effect on the separator is magnified.

When we set $\lambda = 0$, we saw an even stranger pattern, which was that points tended to not be support vectors only if they belonged to a particular class. This was very strange, and we theorized that we saw the effect because in order for the $\alpha$ values to reflect the amount of effect the point has on the separator, it's necessary to have a substantial L1 regularization term.

\begin{figure}
\centering

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_test_plot_lambda_sup_vectors.png}
	\caption{$\lambda = 1$}
\end{subfigure}

\begin{subfigure}[b]{2.25in}
	\includegraphics[width = 2.25in]{plots/stdev2_test_plot_lambda_0_supvec.png}
	\caption{$\lambda = 0$}
\end{subfigure}
\caption{The impact of $\lambda$ on support vectors. Support vectors are shown as dots, and all other points as plusses.}
\end{figure}

Next, we extended our logistic regression to operate with a Gaussian kernel. We use our validation data to tune the values of $\lambda$ and $\beta$ (to minimize validation error), and report the test errors for the optimal values. We repeat the experiments with the linear kernel in order to demonstrate the effectiveness of the Gaussian kernel. The results are shown in Figure 12.


\begin{figure*}
\centering
\renewcommand*{\arraystretch}{1.5}

\begin{tabular}{c c}
\begin{subfigure}[b]{3.5in}
\centering
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	& \texttt{stdev1} & \texttt{stdev2} & \texttt{stdev4} & \texttt{nonSep}\\
	\hline
	$\lambda$ & 0 & 0 & 0 & 0 \\
	\hline
	$\beta$ & 1 & 0.1 & 0.01 & 1 \\
	\hline
	SVs & 400 & 400 & 400 & 400 \\
	\hline
	Test Error & 0.25\% & 7\% & 21.75\% & 5.5\%  \\
	\hline
	\end{tabular}
	\caption{Gaussian Kernel}
\end{subfigure}
&
\begin{subfigure}[b]{3.5in}
\centering
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	& \texttt{stdev1} & \texttt{stdev2} & \texttt{stdev4} & \texttt{nonSep}\\
	\hline
	$\lambda$ & 0  & 0  & 0  & 0  \\
	\hline
	SVs & 400 & 343 & 278 & 400  \\
	\hline
	Test Error & 1\% & 7\% & 23\% & 49.75\%\\
	\hline
	\end{tabular}
	\caption{Linear Kernel}
\end{subfigure}
\end{tabular}

\caption{The parameters which minimized validation error rates for both the linear kernel and the Gaussian kernel on each data set. The number of support vectors resulting from these parameters and the test error rate are also shown.}
\end{figure*}


As with SVM's, we note that the Gaussian and linear kernels perform very similarly on \texttt{stdev1}, \texttt{stdev2}, and \texttt{stdev3}, while we notice that the Gaussian kernel performs far better on \texttt{nonSep}, as expected.

We also notice a few other things. First, for the Gaussian kernel, all the points are classified as support vectors. This is due to the fact that the $\alpha$ values were all sufficiently similar that there was no way to distinguish between them. Even with the linear kernel, we saw far more support vectors with logistic regression than with did with SVM's. This is not surprising from our discussion earlier, where we noted that it was very difficult to choose a threshold which seemed to properly classify support vectors.

We also note that for all situations, the best value of $\lambda$ was $\lambda = 0$. We know that a value of $\lambda = 0$ would optimize training error, but we also found that it optimized validation error. This is not terribly surprising because the training and validation data turned out to be so similar (with no large outliers).


\section{Multi-Class Classification}

In the previous sections, we dealt with data that needed to be classified as positive or negative. In this section, we will discuss data that needs to be classified as one of $\{1, \hdots, k\}$. In particular, we will deal with the problem of predicting the forest cover type (of which there are 7) of a region based on a 54-dimension feature vector, a problem based on the Kaggle challenge. We will apply both support vector machine and logistic regression techniques.

We were given 15,120 data points, which we randomly split into three sets of size 5,040 to serve as the training, validation, and testing data.

\subsection{Support Vector Machines}

We implemented multi-class support vector machines using the one-versus-the-rest approach. We tested the Kaggle data using both the linear and Gaussian kernel, and found that we were able to achieve a much higher success rate with the linear kernel (for reference, the linear kernel tended to classify about twice as many points correctly). We attributed this difference to overfitting of the training data with the Gaussian kernel.

Moving forward with the linear kernel, we begin training the model on the training set and using the validation error rate to tune the value of $C$. Because solving the quadratic program with the support vector machine takes prohibitive amounts of time when run on our training set of 5,040 points, we used a smaller subset of the training and validation sets to tune $C$ (1,000 points each).

We found that the best value of $C$ was $C = 1$. Figure 13 shows the resulting validation errors for various values of $C$. For this value of $C$, the test error was 35.6\% (where the test set was also a random choice of 1,000 points of the full test set).

\begin{figure}
\centering
\renewcommand*{\arraystretch}{1.5}
\begin{tabular}{| c | p{.3in} | p{.3in} | p{.3in} | p{.3in} | p{.3in} |}
\hline
$C$ & \centering 0.01&  \centering 0.1 & \centering 1 & \centering 10 & 100 \\
\hline
VE & 41.1\%& 37.0\% & 36.9\% & 37.2\% & 37.2\% \\
\hline
\end{tabular}
\caption{Validation Error for various values of $C$}
\end{figure}

Due to time constraints, we did not explore any other approaches for handing the fact that our algorithm was not able to efficiently train on the entire training set. One possible option would have been to have performed the computation on a more powerful machine (such as Amazon Web Services). With 8GB of RAM, the machine on which we did the training was completely overtaken by performing the training on the full training set. Another option would have been to abandon our implementation of the SVM and use another implementation which supported Sequential Minimal Optimization, in order to provide an approximate solution in a much more manageable timeframe.

Once we settled on a value of $C$, we performed training on all 5,040 training points. The computation managed to terminate after a few hours. With this model, we were able to achieve a test error rate of 38.3\%, corresponding to a success rate of 61.7\%. We note that the original data contained equally many points in each class, so this is far better than the 14.3\% success rate that we would expect from a model which did not actually have any information about the data set.

We notice that the test error rate was actually better for the smaller sets than the full data set. We speculate that the value of $C$ which we chose based on training the small data set was not the optimal value for the larger data set. In future work, we would like to optimize for $C$ using the full training and validation sets, to see if that allows us to achieve better performance on the full data set.

\subsection{Logistic Regression}

The extension of logistic regression into the multi-class case is somewhat more sophisticated than one might expect, unfortunately, because of the fact that we must now introduce an additional dimension into the problem: the number of classes. Two classes was kind of a special case, where we differentiate nicely between positive and negative; with three or more classes, that is no longer possible. 

The way we classify now ``feels'' a bit different, although at its core it is the same (if one were to use this method of classifying for two classes only, one would find that it would behave identically). Suppose we are trying to determine which of $K$ different classes a given data point belongs to. If we are given some $d$-dimensional input data point $\phi$, and we have some $N \times d$ weight matrix $W$ with which we make our classification decisions, then for each possible class $\phi$ might belong to, we assign to it an ``activation'' $a_k$: a scalar value which indicates in some absolute sense how likely that class is to have been the class $k$ corresponding to $\phi$ a posteriori. We compute $a_k$ via the following dot product: $a_k = W_k^T\phi$, where $W_k$ is the the $k^{\textrm{th}}$ row of $W$. 

Using the activations $a_k$, our estimated probability that $\phi$ is class $k$ is equal to $\frac{e^{a_k}}{\sum_{j=0}^K e^{a_j}}$. 

Let $T$ be the $N \times K$ matrix that contains, for each data point, a 1 in column $k$ if that data point belongs to that class, and a 0 in column $k$ if it does not. Then, the negative log likelihood of our model, therefore, is 

$$-\sum_{n=1}^N \sum_{k=1}^K T_{nk} \ln y_{nk}$$.

We can do a gradient descent on this quantity. \\

We used this method to classify the data points in the Kaggle data. Because we were not validating any parameters, we trained on 10,080 points (the training and validation sets), and then used the same test set for testing. While running our algorithm, the gradient descent function never terminated. However, if the gradient descent was truncated at some point before completion, the result was a weight matrix that classified the points decently, though not as well as SVMs. We did not include L1 regularization, due to its previous lack of effect on the quality of the results but its dramatic effect on the computation time.

\renewcommand*{\arraystretch}{1.5}
\begin{figure}
\centering
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	Iterations & 1 & 10 & 20 & 100 \\
	\hline
	Test Error & 59\% & 44\% & 47\% & 49\% \\
	\hline
	\end{tabular}
	\caption{Test error as a function of the number of iterations before truncation of the gradient descent}
\end{figure}

In Figure 14, we see the effects of the numbers of iterations that we allowed the gradient descent function to run for on the test error. It appears that the gradient descent beyond the first few iterations was largely ineffective at reducing the test error. As we can see, the best that this model was able to achieve was a 56\% success rate (although this is slightly unfair, as we are citing the number of iterations which minimized the test data).

For context, each additional iteration took roughly 20 seconds to compute. Because logistic regression can get decent predictions without taking much time, they have the edge over the SVMs when efficiency is a very serious consideration; however, we found that for the purpose of the competition, the SVM's substantial edge in accuracy over logistic regression made it the more effective choice.

\subsection{Discussion}

We were able to achieve the highest level of accuracy using the multi-class SVM. This is especially promising because, as stated in the previous section, we believe that with more time and/or computational power we could further improve the performance of the multi-class SVM.

Logistic regression did not perform quite as well, but took significantly less time to perform training. To train on the same set of data, logistic regression took about two orders of magnitude less time. For a model which performed only slightly worse than the SVM, this time difference is compelling.







\end{document}