\documentclass{sigchi}

% Use this command to override the default ACM copyright statement (e.g. for preprints). 
% Consult the conference website for the camera-ready copyright statement.


% Arabic page numbers for submission. 
% Remove this line to eliminate page numbers for the camera ready copy
%\pagenumbering{arabic}



\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{amsmath}
%\usepackage{multirow}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, 
% to give it a fighting chance of not being over-written, 
% since its job is to redefine many LaTeX commands.
%\usepackage[pdftex]{hyperref}
%\hypersetup{
%pdftitle={L@S 2014 Work-in-Progress Format},
%pdfauthor={LaTeX},
%pdfkeywords={SIGCHI, proceedings, archival format},
%bookmarksnumbered,
%pdfstartview={FitH},
%colorlinks,
%citecolor=black,
%filecolor=black,
%linkcolor=black,
%urlcolor=black,
%breaklinks=true,
%}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}

\begin{document}

\title{Making the Grade}
\large

\numberofauthors{2}
\author{
  \alignauthor Casey O'Brien\\
    \email{cmobrien@mit.edu}
\alignauthor Adam Yedidia\\
\email{adamy@mit.edu}
}

\maketitle

\renewcommand*{\arraystretch}{1.5}

\section{Introduction}

Say that you are a student in a large introductory class, like 6.006. Partway through the semester, you'd really like to know: \textit{How likely is it that I get an A in this class?}

As TAÕs for 6.006, we field questions like this all the time. We answer them with a very practiced response, something like: \textit{We cannot make any predictions about final grades before all assignments are graded}. We say this because we do not want to risk making an incorrect prediction and having the student complain at the end of the semester. However, this is basically a blatant lie. In fact, we think that given a student's grades partway through the semester, we can make a fairly accurate prediction as to what their final grade in the course will be.

Our work will focus on 6.006, \textit{Introduction to Algorithms}. For the past five years, very little about the class has changed. Each semester, there are 9 assignments: 6 problem sets, 2 quizzes, and a final exam. A student's final numerical grade is weighted as 5\% per problem set, 20\% per quiz, and 30\% for the final exam.

There are three different types of problem that we will discuss in this paper.
\begin{enumerate}
\item \textit{Numerical Prediction}: Predict the student's final numerical average.
\item \textit{Letter Prediction}: Predict the student's final letter grade.
\item \textit{Letter Distribution}: Find the probability distribution over a student's final letter grade.
\end{enumerate}

We solved all of these problems at each point throughout the semester (i.e., after each assignment was due). This allowed us to see how our predictive power increased as we progressed throughout the semester. As one might expect, we saw the largest decreases in our error rates after each of the quizzes.

To compute the numerical prediction, we used regression. For letter prediction, we tried two different models. The first was to perform regression, and then map numerical scores to letter grades. The second was to use an ordered logit/probit model, which, in turn, solved the letter distribution problem. Inspired by the the way that ordered logit/probit computed distributions over the data, we came up with a method for solving the letter distribution problem based on the solution to the numerical predication problem. These models will be discussed in more detail in the coming sections.

\subsection{Data}

We have grade data for six semesters of 6.006 over the past five years. For each student, we have their grade on each assignment, as well as their final letter grade in the class. Within any semester, professors used hard thresholds for letter grade divides. So, if Alice and Bob took 6.006 in the same semester, and Alice had a higher weighted average than Bob, then Alice's letter grade must be at least as good as Bob's.

Figure 1 displays information about our dataset. In particular, Figure 1(a) shows the distribution of students over the different majors (note that double majors were counted in both corresponding columns). We can see that almost all students are either courses 6 or 18. Figure 1(b) shows the gender distribution (which is about 2:1 male:female). Figure 1(c) shows the year distribution, where we see that the students are mostly sophomores. Finally, Figure 1(d) shows the distribution over final grades, which is about 41\% A's, 41\% B's, and 18\% C's. Because the number of $D$'s and $F$'s was so sparse, we considered them the same as $C$'s. So, throughout this paper, $C$'s actually refers to $C$'s, $D$'s, and $F$'s. Note that this distribution varies slightly across different semesters.

\begin{figure*}[t!]

\begin{tabular}{c c c}
\multicolumn{3}{ c }{
	\begin{subfigure}[b]{7in}
		\includegraphics[width=7in]{plots/major.png}
		\caption{The major distribution. Double majors are counted in both columns.}
	\end{subfigure}
	}
\\ &&\\
\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/gender.png}
	\caption{The gender distribution.}
\end{subfigure}
&
\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/year.png}
	\caption{The class year distribution.}
\end{subfigure}
&
\begin{subfigure}[b]{2.25in}
	\includegraphics[width=2.25in]{plots/grade.png}
	\caption{The grade distribution.}
\end{subfigure}
\end{tabular}

\caption{Each subfigure shows the distribution over some personal information about the students in our data set. We can see that students are primarily sophomore males in course $6$, and most receive an $A$ or $B$ in the class.}

\end{figure*}

We would like for our models to be able to predict scores not just in these six semesters, but in future semesters. When this is being used in the future, the test data will consist of students from a single semester, and the goal will be to predict their final grades before we actually know them. As such, we randomly chose a single semester of the six to represent our test data. The test data consists of 196 students. The other five semesters, consisting of 938 students in total, were combined and randomly split into training and validation data.

\subsection{Features}

The first, most obvious features to include were the assignment grades. For each assignment (of the assignments being considered at the time), we included as a feature the difference between the student's score and the mean.  We considered dividing by the standard deviation of the assignment (to get the $z$-score), but we realized this has a key flaw. The weighted sum of the $z$-scores of assignments is not equal to the $z$-score of the weighted sum of raw scores. The result is that when students are sorted by weighted sum of $z$-scores, we no longer have the property that grades obey hard thresholds.

This is perhaps an interesting issue to consider about the way that we assign grades. Say that Alice scores 20 points below average on a test with a very high variance and 20 points above average on a test with very low variance. Bob, on the other hand, scores 20 points above average on the high variance test and 20 points below average on the low variance test. It seems clear that Alice is performing better in the class, yet by our current grading scheme, both students have the same overall score.

Of course, since all of the semesters for which we have data were not graded using $z$-scores, we do not consider this idea any further here.

In order to make it easier to comprehend the weights resulting from our models, we scaled problem set features to be out of 5 (before subtracting the mean), quiz features out of 20, and the final feature out of 30. The result is that when we compare the weights for say, a problem set and a quiz, we can directly compare the effects of a $1\%$ change to final score caused by a problem set and a $1\%$ change to final score caused by a quiz.

Another feature we included is the number of zeros scored on problem sets. For various reasons, some students do not complete certain problem sets throughout the semester. If we are told that a student has an average of $50\%$ on problem sets after the first two problem sets, we might expect that their future problem set scores be about $50\%$. However, if we know that their first problem set score was a $0\%$ and their second a $100\%$, we probably expect their future problem set scores to be much higher than $50\%$.

We also included a feature indicating whether the student took the class in the fall or the spring. While we would not expect to see a huge weight corresponding to this feature, it is interesting to see how it affects our predictions.

For the remainder of our features, we included personal data about the students. To obtain this information, we wrote a web scraper to pull the information from the MIT Alumni Database and MIT People. For each student, we recorded their gender, graduation year, and major.

For five students, we were unable to locate their information. For these students, we guessed their gender (with fairly high confidence) based on their names, and assigned their graduation year and major based on the most likely values. We could have used a more sophisticated method here, but because there were so few students in question, it would have been very unlikely to make any difference.

To capture gender in a feature, we included a binary variable indicating if the student was male. To capture graduation year, we included a feature containing the undergraduate school year in which the student took the class (e.g., 1, 2, 3, or 4). The few graduate students in the class were considered a 5. Finally, to capture major, we included three binary features, indicating if the student was course 6, course 8, or course 18. Students with double majors had both corresponding features on. While we would have liked to consider the binary features for all other majors, the data was too sparse for us to believe we could get a real signal from it.

\subsection{Roadmap}

In the next section, we will discuss our implementation of regression, and how it allowed us to solve the numerical prediction and letter prediction problems. In the following section, we will discuss the ordered logit and probit models, and how they solved the letter prediction and letter distribution problems. In the final section, we will present a method for solving the letter distribution problem using the results of the numerical prediction problem.

\section{Regression}

Linear regression is a very natural approach for solving the numerical prediction problem. The numerical prediction problem involves predicting a single continuous value (a student's final score) as a function of the student's past grades in the class, and personal information about the student, such as year, gender, and major. 

It seems very plausible that the student's final score depends \emph{linearly} on the feature values. Consider grades, for example; we can separate the effect of previous grades on final score into a direct effect and an indirect effect. The direct effect of past grades on final score represents the fact that higher previous grades give you a better final score simply because the final score is partly based on the previous grades. Plainly, this effect is linear (it says so right in the syllabus!).

The indirect effect of past grades on final score represents the fact that one's past performance in the class is predictive of future performance. It is less obvious that this effect is linear. But we claim that the best prediction of how a student will do in the class in the future, to first order, is how they've done in the past.

If we want a more precise understanding of the best prediction of how a student will do in the class in the future, we have to account for regression to the mean. A student who has done extremely well on the first assignment is unlikely to do as well on the second assignment, because odds are their early overperformance was in part due to good fortune. We will argue here, however, that the best prediction of a student's future performance is still linear in their past performance, even if the two aren't equal.

Our argument proceeds as follows. We assume that each student has some level of inherent ``talent'' $t$ that affects their performance on assignments. We assume that talent is normally distributed among the students in the class, with standard deviation $\sigma_t$. 

We assume that a student's performance on assignments is going to be equal to their talent, along with some additive Gaussian noise with standard deviation $\sigma_s$. Let $S$ be the random variable representing this student's performance on an assignment, and $T$ be a random variable representing the student's talent. The student's probability of getting some score $s$ on that assignment is then given as follows (all scores, talents, and additive noise are centered around 0):
%
$$Pr(S = s) = \frac{1}{\sqrt{2\pi(\sigma_s^2 + \sigma_t^2)}}\cdot \exp\left \{\frac{-s^2}{2(\sigma_s^2+\sigma_t^2)} \right\}$$
%
However, this is not exactly what we want to know. What we want to know is, given that the student got some score $s$ on a past assignment, what is the likeliest value for their talent to have? The likelihood that they got a score $s$ on an assignment, as a function of $t$ and $s$, is given by
%
$$Pr(S = s) \sim Pr(T = t) Pr(S = s | T = t)$$ 
$$Pr(S = s) \sim \exp\left \{\frac{-t^2}{2\sigma_t^2}\right \}\cdot \exp\left \{\frac{-(s-t)^2}{2\sigma_s^2}\right \}$$
%
We want to find the value for $t$ that maximizes the likelihood. To do this, we take the derivative with respect to $t$ of the above likelihood, and set it equal to zero.

\begin{equation*}
\begin{split}
& \frac{d}{dt} \left(\exp\left \{\frac{-t^2}{2\sigma_t^2}\right \}\cdot \exp\left \{\frac{-(s-t)^2}{2\sigma_s^2}\right \} \right) \\
&=  \left (\frac{t}{\sigma_t^2} + \frac{t-s}{\sigma_s^2}\right )\\
& \times \left (\exp\left \{\frac{-t^2}{2\sigma_t^2}\right \}\cdot \exp\left \{\frac{-(s-t)^2}{2\sigma_s^2}\right \} \right)\\
\end{split}
\end{equation*}
We can see that this will only ever be 0 when we have
\[\frac{t}{\sigma_t^2} + \frac{t-s}{\sigma_s^2} = 0.\]
Solving for $t$ yields
\[t = s \frac{\sigma_t^2}{\sigma_s^2+\sigma_t^2}.\]
Talent does indeed depend linearly on past exam score, with a constant multiplicative factor of $\sigma_t^2/(\sigma_s^2+\sigma_t^2)$, representing the effect of regression to the mean. 

Because likely future score plainly depends directly on talent, it follows that future score will depend linearly on past score, according to our assumptions. We therefore argue that our use of linear regression in this case is justified.

%In fact, it's roughly this times 2/3, because of regression to the mean; and this estimate, of course, ignores the student's personal information. Assuming that a student's performance on assignments is their ``talent'' plus some additive Gaussian noise, and talent is normally distributed in the course, talent will depend linearly on past performance, which implies that future performance will depend linearly on past performance.

Students' final scores can't help but depend linearly on binary variables such as gender, or membership in a major (just from the fact that the relationship to any binary variable must be linear). The notion that students' final scores depend linearly on their year, however, is more dubious.

One might realistically imagine that being a freshman is different from being a sophomore, junior, or senior, for instance, because of the special grading options offered to freshmen, or because taking 6.006 as a freshman signals a particular enthusiasm for the study of algorithms that taking 6.006 as a sophomore, junior, or senior simply might not indicate.

We looked at the average scores of freshmen, sophomores, juniors, and seniors, and found that the A/B/C distribution went 61\%/24\%/14\% for freshmen, 44\%/42\%/14\% for sophomores, 32\%/39\%/29\% for juniors, and 30\%/30\%/39\% for seniors. This progression is perhaps not exactly linear. We felt, however, that it was close enough to linear to warrant inclusion of year as a continuous variable in its own right, and not as four binary variables.

Because all of the features have a linear or nearly-linear relationship with the number we seek to predict, linear regression is a good tool to use to solve this problem.

\subsection{Implementation}

Our implementation uses ridge regression to predict students' numerical final scores as a function of the feature vectors. We make use of the ordinary least-squares method for solving a matrix equation to get the vector of weights. Recall that the we want to minimize the sum of squared errors plus a regularization term. Namely, we want to solve
%
\[\min_{w, w_0} \sum_{i = 1}^{n} (x^{(i)} \cdot w + w_0 - y^{(i)})^2 + \lambda ||w||^2,\]
where $n$ is the number of students in our data set.
% and let $d$ be the dimensionality of our feature vectors. (If we were doing prediction knowing only the scores on the first three assignments, $d$ would be 8: three features for being course 6, course 8, or course 18, one feature for gender, one feature for year, and one feature for each of the three assignments.)

Say that we have $d$ features. Let $X$ be the $n \times d$ matrix of feature vectors (not including a row of ones), and let $Y$ be the be the $n \times 1$ vector of true final scores. Define $Z$ to be the centralized data matrix, obtained by subtracting the mean of each column of $X$ from every entry in that column. Let $\bar{x}$ be the vector of column means. Similarly, define a centralized result matrix $Y_c$ by subtracting from each entry of $Y$ the average value of $Y$, which we will call $\bar{y}$.

%Let $X'$ be the feature matrix, but with 1's (in order to be able to properly calculate the offset $x_0$) prepended to each of the matrix's rows. $X'$ is therefore an $n \times (d+1)$ matrix. \\

%Let $Z$ be the \emph{centralized} data matrix; in other words, just as $X$ is a matrix of feature vectors $x^{(i)}$, $Z$ is a matrix of normalized feature vectors $z^{(i)}$, where each vector $z^{(i)}$ is defined in terms of $X$ as follows: 

%$$z^{(i)} = x_j^{(i)} - x_j$$

%Similarly, we defined a centralized result matrix $Y_c$ with

%$$y_c^{(i)} = y^{(i)} - \bar{y}$$

As we've derived before, the optimal solution to this equal is given by
$$w = (Z^T Z + \lambda I)^{-1} Z^T Y_c$$
It remains to solve for $w_0$. To do so, we can use
$$W_0 = (\bar{y} - w \cdot \bar{x}).$$
We implemented this solution using Python.

\subsection{Results}

We performed ridge regression on our training data, using the validation data to optimize $\lambda$. Figure 2(a) shows the resulting MSE as a function of the number of assignments used as feature vectors (for now, ignore the green line in this Figure). To properly read this graph, keep in mind the order of assignments:
\[PS1, PS2, Q1, PS3, PS4, Q2, PS5, PS6, F\]
We can see that as we include more assignments, our MSE decreases, with the most noticeable decreases after each of the quizzes.

\begin{figure*}[t!]
\centering
\begin{tabular}{c c}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1.png}
	\caption{MSE v. number of assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/2.png}
	\caption{Number of errors v. number of assignments}
\end{subfigure}
\\
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/3.png}
	\caption{MSE v. percentage of final grade}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/4.png}
	\caption{Number of errors v. percentage of final grade}
\end{subfigure}
\end{tabular}
\caption{The left column shows the value of the MSE, while the right column shows the number of errors in the test set. The top row shows these values as a function of the number of assignments included as features, and the bottom rows shows the values as a function of percentage of the final grade included as features. All plots show results for both when we were minimizing MSE and when we were minimizing number of errors.}
\end{figure*}

This graph is slightly misleading, as it should obviously be the case that the assignments which make up the largest percentages of their grades give us the most information. In Figure 2(c), we see the same curves, but plotted against the percentage of the final grade which had been determined after that assignment.

We notice that the amount that our error decreases is not quite linear in the number of percentage points which we include in the features. This implies that as we know more about student grades, each additional percentage point tells us less information than the previous. It is also interesting to note that problem set percentage points improve our predictions by just as much as quiz percentage points. In other words, the amount by which our error decreases after a problem set appears to be about one-quarter the amount by which our error decreases after a quiz.

\subsubsection{Predicting Letter Grades}

As noted bofore, regression is extremely well-suited to predicting numerical scores. Predicting letter grades, however, is perhaps a slightly more awkward fit, since linear regression is not properly designed for classification.

In order to use linear regression to predict letter grades, the simplest conversion is to take the predicted final scores returned by the ridge regression as described above, and sort the scores into $A$, $B$, and $C$ bins according to the proportion of $A$'s, $B$'s, and $C$'s from the training and validation data.

In the training and validation data, we find that there are $41.0\%$ $A$'s, $41.7\%$ $B$'s, and $17.3\%$ $C$'s. Thus, we take the final scores predicted by the ridge regression, and we assign the predicted scores of the students to letter grades according to whether they belong to the top $41.0\%$ of predicted scores (in which case we predict an $A$), the bottom $17.3\%$ of predicted scores (in which case we predict a $C$) or the middle $41.7\%$ of predicted scores (in which case we predict a B).

Using this method, we do reasonably well, misclassifying 8 students from the test set when we have access to every assignment grade and 126 students when we have access to none of them. The misclassification rate of this algorithm throughout the semester is displayed as a function of the number of assignments used as features in Figure 2(b). As before, we also show the same curve plotted against the percentage of the final grade determined at that point (in Figure 2(d)).

The trends in the number of misclassified points is very similar to that of MSE. We notice that the curve is slightly more jagged, but this is an obvious ramification of the fact that this measure is not as smooth as MSE.

\subsubsection{Minimizing Number of Errors}

While using this approach seems to work reasonably well, it is somewhat unnatural. The weight vector is being optimized to minimize MSE, but is being used to predict letter grades. It would seem to make more sense to optimize the weight vector to minimize the number of errors in letter grade classification, if that is what it is going to eventually be used for. Therefore, we seek to solve
%
$$\min_{w, w_0} \sum_{i=1}^n I\{\hat{y}^{(i)} \neq y^{(i)}\} + \lambda ||w||^2,$$
where $\hat{y}^{(i)}$ represents the grade that we predict for the student based on sorting them by final score and assigning grades according to percentages in the training and validation sets. Note that throughout the paper we use $y^{(i)}$ to mean both the students final numerical score and their letter grade. It will be obvious from the context which we are referring to.

This error function is not nicely differentiable the way the ordinary objective function for ridge regression is. Therefore, we could not find a closed form for the solution. Instead, we performed a gradient descent over $w$ and $w_0$ in an attempt to find a local minimum whose values will hopefully not be far from those of the global minimum. Again, we implemented this in Python, using \texttt{fmin\_bfgs} for gradient descent.

To our mild surprise, the results of the gradient descent did not yield materially different results from the MSE minimization, either in terms of the observed MSE (which would have expected the previous method to perform better at) or in terms of the number of errors in classification in the test set (which we would have expected the previous method to perform worse at). The green lines in Figure 2 represent all the same results as when we were minimizing MSE, but using the weight vectors obtained from minimizing the number of incorrectly classified points.


\begin{figure}[t!]
\centering
\begin{tabular}{| c || c | c | c | c |}
\hline
 &\multicolumn{4}{ c |}{Weights}\\
\hline
\# Assignments & 0 & 3 & 6 & 9 \\
\hline
$w_0$ & $5.04$ & $-1.00$ & $-0.07$ & $0.00$ \\ 
\hline
Problem Set 1 & $-$ & $2.44$  & $1.11$  & $1.00$ \\
\hline
Problem Set 2 & $-$ & $4.04$  & $0.85$ & $1.00$ \\
\hline
Quiz 1	      & $-$ & $2.49$  &  $1.53$ & $1.00$ \\ 
\hline
Problem Set 3 & $-$ & $-$          & $1.94$ & $1.00$ \\
\hline 
Problem Set 4 & $-$ & $-$          & $1.99$ & $1.00$  \\
\hline
Quiz 2	       & $-$ & $-$          & $1.79$ & $1.00$ \\
\hline
Problem Set 5 & $-$ & $-$          & $-$& $1.00$\\
\hline
Problem Set 6 & $-$ & $-$          & $-$& $1.00$ \\
\hline
Final		& $-$       & $-$          & $-$& $1.00$\\
\hline
Pset-zeros   & $-$ & $4.89$ & $-0.08$ & $0.00$ \\
\hline
Fall		&  $0.62$     & $0.98$ & $1.49$ & $0.00$ \\
\hline
Male		&  $1.00$     & $1.41$  & $0.26$ & $0.00$ \\
\hline
Year 		&  $-2.82$   & $-1.06$ & $-0.16$ & $0.00$\\
\hline
Course 6	&  $-0.34$    & $1.80$  & $0.03$ & $0.00$ \\
\hline
Course 8	&  $0.45$  &  $1.57$  &  $0.40$  &  $0.00$ \\
\hline
Course 18       & $2.00$ & $2.58$  & $0.17$ & $0.00$ \\
\hline
\end{tabular}
\caption{Each column represents the weights returned by ridge regression with the optimal value of $\lambda$, using the number of assignments specified in the column head as features. }
\end{figure}

\subsubsection{Feature Weights}
Since ridge regression seemed to provide better results across the board, we proceed with ridge regression from this point forward. Next, we take a look at the actual weights produced by ridge regression, to see how important it believes the various features are.

Figure 3 shows the resulting weight vectors from using ridge regression at various points throughout the semester. Specifically, it shows the results after 0 assignments (based only on personal information), 3 assignments (right after the first quiz), 6 assignments (after the second quiz), and 9 assignments (after the final). 
%we got the following values for the weight vector's entries. Note that we ran the regression after knowing 0, 3, 6, and 9 of the assignments' values, respectively, before predicting final score. 

Looking at the results, there are a few noteworthy trends. The first is that weights tend to decrease in absolute value as one progresses across the table from left to right; the weights for assignments tend towards one, and the weights for personal statistics tend towards zero.

We theorized that assignments would have weights of more than one and personal statistics would have non-zero weights because both the early assignment scores and the personal statistics have some predictive power over future assignments. As the number of future assignments decreases, then, one would naturally expect this predictive effect to decrease and the direct effect of assignment grades being factored into final grades to increase.

In addition to this overall effect on the columns of the table, we can observe some interesting trends on rows. Taking the class in the fall, being male, and being a physics major all appear to give a small advantage in 6.006; being a math major, and being a freshman rather than a junior or a senior both appear to confer a somewhat more important advantage.

All of these effects, of course, are dwarfed by the predictive effect of early assignments on later ones. This is entirely unsurprising; as discussed earlier, the best predictor of final score is, to first order, performance up until this point.

%There is one more feature that deserves mention here, and that is the effect on score of getting zeros on problem sets. We theorized early on that getting zeros on problem sets would actually have a substantial \emph{positive} effect on predicted score, because a zero on a problem set does not carry the same predictive weight that poor scores on assignments otherwise might. Put otherwise, most low scores on assignments tend to signal a relatively poor understanding of the material, whereas a zero on a problem set might instead be signaling some unlikely event which prevented the student from completing that particular problem set. We see that this theory is more or less borne out by the data, with the positive effect of problem set zeros beginning strong and quickly becoming very weak as the predictive effect of problem sets diminishes with time.

%However, this approach is somewhat unnatural, in that the weight vector is being optimized to minimize MSE, but is being used to predict letter grades. It would seem to make more sense to optimize the weight vector to minimize the number of errors in letter grade classification, if that is what it is going to eventually be used for. Therefore, we seek to minimize the following error function:

%$$E(w) = \sum_i e^{(i)} + \lambda ||w||^2$$

%where $e^{(i)} = 1$ if $c(x^{(i)} \cdot w) = y^{(i)}$ and $e^{(i)} = 0$ if $\mu(x^{(i)} \cdot w) \not{=} y^{(i)}$, with $\mu(s)$ defined as above.    \\

%This error function is not nicely differentiable the way the ordinary goal function for ridge regression is. Therefore, we could not find a closed form for the solution; instead, we performed a gradient descent over the $w$ values, in an attempt to find a local minimum whose values will hopefully not be far from those of the global minimum. To do this gradient descent, we used the fmin\_bfgs package from scipy.\\

%To our mild surprise, the results of the gradient descent did not yield materially different results from the MSE minimization, either in terms of the observed mean-squared-error (which would have expected the previous method to perform better at) or in terms of the number of errors in classification in the test set (which we would have expected the previous method to perform worse at). The new method misclassified 14 students with access to every assignment grade and 126 students with access to no assignment grades. A more complete description of its performance is visible in the figure below. % TODO deal

\section{Ordered Logit/Probit}

The problem of grade prediction lends itself more naturally to classification than to regression. Thus, we decided to model this problem as a classification problem. Now, we are no longer interested in predicting students' final numerical scores. Instead, we are only interested in predicting students' final letter grade.

Given a student's grades and information partway through the semester, we want to predict their final letter grade. This can easily be modeled as a multi-class classification problem. However, the problem is actually more structured than that, because our class values, $A$, $B$, and $C$, have an inherent ordering. Thus, we make use of ordered logit and ordered probit.

These techniques are very similar, so at first we will not differentiate between them. The goal is to map the feature vector, which we will call $x$ for simplicity, to a real number. We will do this by solving for a weight vector $w$, and then mapping $x$ to $x \cdot w$. Then we want to choose cutoffs $\mu_A$ and $\mu_B$ which separate the $A$'s, $B$'s and $C$'s. Once we have solved for $w$, $\mu_A$, and $\mu_B$, we can predict a final grade $y$ as follows

\begin{equation} y=
\begin{cases} 
      A &  \mu_A \leq x \cdot w\\
      B & \mu_B \leq x \cdot w < \mu_A \\
      C & x \cdot w < \mu_B 
   \end{cases}.
\end{equation}

Note the lack of $w_0$ term above. We can see that if we chose to include a $w_0$ term, it would simply result in each $\mu$ being offset by $w_0$ as well, leading to an equivalent result. Thus, without limiting our model, we can omit $w_0$. Next, we see how the two methods differ.

\begin{figure*}[t!]
\centering
\begin{tabular}{c c}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/pdf.png}
	\caption{PDF}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/cdf.png}
	\caption{CDF}
\end{subfigure}
\end{tabular}
\caption{The CDF and PDF of Gaussian and logistic distributions with mean $0$ and standard deviation $1$.}
\end{figure*}


\subsection{Ordered Probit}

We will first describe the ordered probit model. Say that we assume that a student's final score will be distributed as a Gaussian around their current scores with standard deviation $\sigma$. If we knew the thresholds $\mu_A$ and $\mu_B$, we could calculate the probability that a student received an $A$ in the class as
\[P(y = A) = \Phi \left (\frac{x \cdot w - \mu_A}{\sigma} \right ).\]
Similarly, we have
\begin{equation*}
\begin{split}
P(y = B) & = \Phi \left ( \frac{\mu_A - x \cdot w}{\sigma} \right ) - \Phi \left (\frac{\mu_B - x \cdot w}{\sigma} \right ) \\\
P(y = C)  &= \Phi \left (\frac{\mu_B - x \cdot w}{\sigma} \right ).
\end{split}
\end{equation*}
From these equations, it is easy to see that if $x \cdot w > \mu_A$, then $P(y = A) > P(y = B) > P(y = C)$. So, whenever we have $x \cdot w > \mu_A$, the maximum likelihood estimate will be $A$. Using similar reasoning for when $x \cdot w < \mu_A$, we can see that equation (1) is simply saying to choose the maximum likelihood estimate.



This works out nicely, but we still need to solve for $w$, $\mu_A$, and $\mu_B$, and now $\sigma$ as well. First, we note that we can set $\sigma = 1$, and it will not restrict our model at all, as we can scale $w$, $\mu_A$, and $\mu_B$ accordingly. So now we have
\begin{equation*}
\begin{split}
P(y = A) &= \Phi \left (x \cdot w - \mu_A \right )\\
P(y = B) &= \Phi \left (\mu_A - x \cdot w \right ) - \Phi \left (\mu_B - x \cdot w \right )\\
P(y = C) &= \Phi \left (\mu_B - x \cdot w \right ).
\end{split}
\end{equation*}
Of all the students that we eventually assign an $A$, we would like to maximize the likelihood that they receive an $A$, and similarly for $B$'s and $C$'s. So, our goal will be to solve the following optimization:
\[\max_{w, \mu_A, \mu_B} L(w, \mu_A, \mu_B),\]
where
\begin{equation*}
\begin{split}
L(w, \mu_A, \mu_B) &= \prod_{\hat{y}^{(i)} = A} P(y^{(i)} = A)\\
&\times \prod_{\hat{y}^{(i)} = B} P(y^{(i)} = B)\\
&\times \prod_{\hat{y}^{(i)} = C} P(y^{(i)} = C)
\end{split}
\end{equation*}
To solve this problem, we instead try to minimize the negative log-likelihood. So, our optimization problem is now
\[\min_{w, \mu_A, \mu_B} -\log{L(w, \mu_A, \mu_B)},\]
where
\begin{equation}
\begin{split}
-\log{L(w, \mu_A, \mu_B)} =& -\sum_{\hat{y}^{(i)} = A} \log{P(y^{(i)} = A)}\\
&- \sum_{\hat{y}^{(i)} = B} \log{P(y^{(i)} = B)}\\
&- \sum_{\hat{y}^{(i)} = C} \log{P(y^{(i)} = C)}.
\end{split}
\end{equation}


\subsection{Ordered Logit}

The ordered logit model is very similar to the ordered probit model. The difference is that instead of assuming that future grades are distributed as a Gaussian, we assume that they are distributed according to a logistic distribution. Figure 4 shows the differences between the Gaussian distribution and the logistic distribution. A logistic distribution is something like a Gaussian distribution with ``fat tails''.

The cumulative distribution function for the logistic distribution is given by
\[F(x) = \frac{1}{1 + e^{-x}}.\]
Thus, to perform ordered logit, we simply need to replace occurrences of $\Phi$ in ordered probit with $F$. So now we have
\begin{equation*}
\begin{split}
P(y = A) &= F \left (x \cdot w - \mu_A \right )\\
P(y = B) &= F \left (\mu_A - x \cdot w \right ) - F \left (\mu_B - x \cdot w \right )\\
P(y = C) &= F \left (\mu_B - x \cdot w \right ).
\end{split}
\end{equation*}
From here, the form of Equation (2) is unchanged.

\subsection{Choosing a Model}

It's difficult to tell which of the ordered probit or logit models will be more suitable for the data. It depends on knowing how future student grades depend on past student grades. To get a sense for this distribution, we graphed it.

Consider the point in the semester after which we know 3 student grades (two problem sets and a quiz). For each student in the training set, we computed our best guess of their average at that point in the semester (using ridge regression). We then subtracted their final numerical average in the class. The result is the amount that their average changed. Figure 5 shows histograms of these values at the beginning of the semester, after 3 assignments, after 6 assignments, and after 9 assignments.

\begin{figure}
\includegraphics[width = 3in]{plots/gaussian.png}
\caption{The distribution over differences between final grades and predicted grades. Each curve corresponds to a different number of assignments used as features to compute the predicted grades.}
\end{figure}

The distributions seem relatively Gaussian, with smaller standard deviations as the semester progresses. This is what we would expect, since later in the semester, much less of the grade is unknown. We also notice that the curves tend to be centered slightly to the left of the origin.

For 6.006, almost all assignment averages are above $50\%$. As a result, it is possible to fail by more than it is possible to succeed. Histograms of assignments tend to be skewed to the right. Our model does not take advantage of this trend.

Clearly, the graph does not contain enough information for us to distinguish between a Gaussian and a logistic distribution. As such, we will apply both ordered logit and ordered probit to our data.

\subsection{Applying To Our Data}

To solve these optimization problems, we implemented ordered logit and probit in Python, by minimizing the objective functions using \texttt{fmin\_bfgs}.

Because ordered logit and probit do not have any parameters we need to use validation data to optimize, for this section we combined our training and validation data, so that our training set has 938 points.

Next, we applied these models to our data in order to predict student letter grades. As before, we applied both of these models after each assignment throughout the semester. The results were very similar to those described in the Regression section.

\begin{figure}[t!]
\centering
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/ordered_num_missed.png}
	\caption{Number of errors v. number of assignments}
\end{subfigure}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/percentage_num_missed.png}
	\caption{Number of errors v. percentage of final grade}
\end{subfigure}
\caption{Depicts the test error as we increase the number of assignments that we include as features. Each graph shows the results using each of the ordered logit and ordered probit models.}
\end{figure}

Figure 6(a) shows the number of misclassified students as a function of the number of assignments included as features. We are not surprised to see that we see the largest drops in number of misclassified students after the third, sixth, and ninth assignments, as these assignments make up the largest proportions of student grades.

Figure 6(a) can be directly compared to Figure 2(b), where we also graphed the number of errors versus number of assignments, for when we were using numerical prediction to solve letter prediction. The graphs are nearly identical, so we can see that using ordered logit/probit to predict grades has about the same error rate as using regression. Similarly, we can directly compare Figures 6(b) and 2(d).

We will also note that not only do these models all make almost exactly the same \textit{number} of errors, but they also tend to misclassify almost the exact same set of students. This is not surprising, given that when students do unexpected things, we tend to make the wrong prediction about them. For example, consider a student who does very well in the class until the final, and then performs very poorly on the final. All of our models will almost certainly misclassify this point.

Note that the most common grade among the training data was a $B$. Thus, we can compare these results to the case where we assign $B$'s to all students in the class. This would result in a test error of 122 (corresponding to a 37.8\% accuracy rate), which is slightly worse than that achieved by ordered logit/probit, even when no assignments are considered. Thus, using just personal information about the students, we are able to do better than the trivial algorithm.


It is somewhat disappointing that we are not able to determine final grades with 100\% accuracy, even when all of their scores are completely determined. The reason for this is variation among thresholds from different professors. If professors observed strict proportions of $A$'s, $B$'s, and $C$'s, we could easily predict grades with 100\% accuracy after the final. Sadly, MIT does not allow such grading policies.

\subsubsection{Feature Weights}

To see how important the models consider the various features, let us look at the weights that they produce for each feature at various points throughout the semester.

Figure 7 shows the weights assigned to each feature before any assignments and after assignments 3, 6, and 9 (note that these mark the points after each quiz/final) for ordered probit.

We first note that this chart should not be directly compared to Figure 3. We are no longer trying to perfectly predict numerical scores. Thus, we do not expect to see a weight vector where all assignments have value 1 and all other features 0. Recall that when performing ordered probit, the weights are essentially scaled to account for the standard deviation of the Gaussian relating future scores to past scores. We also do not use a $w_0$ term in ordered probit.

Despite the lack of direct comparison with regression, we can observe some general trends from looking at these weights. First, the assignment scores tend to carry a lot more weight than the personal information, which is expected.

The weights of all the assignments tend to be very similar. Recall that the values of the features already take into account the fact that different assignments have different weights in their final grades, so we expect these weights to be roughly equal.

We do see some variety among the weights, although we believe this is because there is some noise introduced by the fact that these assignments had different variances (and the variances differed slightly among years). For example, most students perform very well on problem set 1. Thus, a student's score on problem set 1 does not give us much information about their final grade. We believe this is why we tend to see a much lower weight on the first problem set.

A few of these weights have very unexpected values in the end. For example, the fact that the course 6 feature weight is higher than the feature weight on the final exam when all 9 assignments are considered is certainly unnerving. We attribute strange behavior like this to overfitting the training data. While we do not experiment with this idea in this paper, future work might try adding a regularization weight to ordered logit/probit in order to minimize the effects of overfitting.

\begin{figure}[t!]
\centering
\begin{tabular}{| c || c | c | c | c |}
\hline
 &\multicolumn{4}{ c |}{Weights}\\
\hline
\# Assignments & 0 & 3 & 6 & 9 \\
\hline
\hline
Problem Set 1 & $-$ & $0.17$  & $0.16$  & $0.36$ \\
\hline
Problem Set 2 & $-$ & $0.45$  & $0.23$ & $0.46$ \\
\hline
Quiz 1	       & $-$ & $0.49$  &  $0.45$ & $0.49$ \\ 
\hline
Problem Set 3 & $-$ & $-$          & $0.53$ & $0.44$ \\
\hline 
Problem Set 4 & $-$ & $-$          & $0.39$ & $0.30$  \\
\hline
Quiz 2	       & $-$ & $-$          & $0.38$ & $0.35$ \\
\hline
Problem Set 5 & $-$ & $-$          & $-$& $0.62$\\
\hline
Problem Set 6 & $-$ & $-$          & $-$& $0.28$ \\
\hline
Final		       & $-$ & $-$          & $-$& $0.50$\\
\hline
Problem Set $0$'s & $-$ & $-0.39$ & $-0.16$ & $0.04$\\
\hline
Fall		       & $0.051$ & $-0.08$ & $-0.20$ & $-0.23$ \\
\hline
Male		       & $0.014$ & $0.07$  & $0.00$ & $0.05$ \\
\hline
Year 		       & $-0.27$ & $-0.10$ & $0.00$ & $0.01$\\
\hline
Course 6	       & $0.17$ & $0.28$  & $0.19$ & $0.49$ \\
\hline
Course 8	       & $0.71$ & $0.26$  & $0.49$ & $0.00$ \\
\hline
Course 18       & $0.13$ & $0.34$  & $0.10$ & $0.15$ \\
\hline
\end{tabular}
\caption{Each column represents the weights returned by ordered probit with the optimal value of $\lambda$, using the number of assignments specified in the column head as features.}
\end{figure}


\subsubsection{Grade Distributions}

Next, we move on to focus on the grade distribution problem. Note that when we solved the ordered logit/probit, we already solved the grade distribution problem. To solve the grade prediction problem, we simply chose the maximum likelihood estimate. However, this is just throwing away additional information.

Consider a student who comes to you, their TA, after quiz 1, wanting to know what their final grade will be. If you, the TA, predict they will get an $A$, and then they end up with a $B$, they will probably be very unhappy with you at the end of the semester. Instead, if you tell them they have a $52\%$ chance of getting an $A$ and a $48\%$ change of getting a $B$, they will probably by much less angry come the end of the semester.

With this in mind, perhaps finding the number of misclassified students is not the correct function to focus on. In fact, this was not the function that we minimized when we solved the ordered logit/probit problem. Recall that our goal was to minimize the negative log likelihood.

While we chose to minimize the negative log likelihood (as opposed to just maximizing the likelihood), primarily for mathematical convenience, the value of this function has some interesting properties. In some sense, we can think of it as a penalty function for an assignment of distributions. A lower penalty corresponds to a better distribution. 

\begin{figure}[t!]
\centering
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/ordered_score.png}
	\caption{NLL v. number of assignments}
\end{subfigure}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/percentage_score.png}
	\caption{NLL v. percentage of final grade}
\end{subfigure}
\caption{Depicts the value of the minimum negative log likelihood as we increase the number of assignments used as features. Each graph shows the results using each of the ordered logit and ordered probit models.}
\end{figure}

We notice that if we assign a student a $0\%$ chance of receiving a grade that they eventually receive, then our penalty is $\infty$. This is desirable, because if we are ever so sure that a student has a $0\%$ chance of receiving a grade, and we end up being wrong, our predictor is clearly doing a very poor job (note that our algorithm will never assign a probability of $0\%$, just arbitrarily small probabilities).

We also notice that if we guess their grade correctly with $100\%$ certainly, we suffer no penalty. This also seems appropriate. If we are entirely sure of a student's final grade, then we should not be penalized for our guess.

Figure 8 shows the minimum values of the negative log likelihoods after each assignment is due, for both ordered logit and probit. Interestingly, they produce nearly identical results. This is good news, because it means that the distributional assumptions are not driving the result. As in Figure 6, we show both the curves as a function of the number of assignments completed (in Figure 8(a)), and as a function of percentage of the grade completed (in Figure 8(b)). 

In Figure 8(b), we also observe that the value of the slope decreases slightly as we determine more of their final grades. Again, this points to the fact that as we move through the semester, each additional determined percentage point tells us less than the previous.

In our training data, we observe $41.0\%$ $A$'s, $41.7\%$ $B$'s, and $17.3\%$ $C$'s. We could trivially assign each student in the test set this probability distribution. In the test set, there are 83 $A$'s, 74 $B$'s, and 39 $C$'s. Using this assignment, the negative log likelihood would be
\[- 83 \cdot \log(0.410) - 74 \cdot \log(0.417) - 39 \cdot \log(0.173)\]
\[ = 207.15\]
Again, we notice that even with no assignments included in the features, our ordered logit/probit is able to do slightly better than the trivial solution.

\subsubsection{Understanding the Distributions}

\begin{figure}
\centering
\begin{tabular}{|c | c | c | c |}
\hline
Student & $P(y = A)$ & $P(y = B)$ & $P(y = C)$\\
\hline
Ben & $28.1\%$ & $66.9\%$ & $5.0\%$\\
\hline
Jeremy & $38.6\%$ & $58.7\%$ & $2.7\%$\\
\hline
Paul & $57.9\%$ & $41.4\%$ & $0.07\%$\\
\hline
\end{tabular}
\caption{The probability distributions predicted by ordered probit for students described in \textit{Understanding the Distributions} section.}
\end{figure}

\begin{figure*}
\centering
\begin{tabular}{c c}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/probit3dist.png}
	\caption{Histogram of ML's for 3 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1pro3.png}
	\caption{Ratio of ML to percentage correct for 3 assignments}
\end{subfigure}
\\
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/probit6dist.png}
	\caption{Histogram of ML's for 6 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1pro6.png}
	\caption{Ratio of ML to percentage correct for 6 assignments}
\end{subfigure}
\\
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/probit9dist.png}
	\caption{Histogram of ML's for 9 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1pro9.png}
	\caption{Ratio of ML to percentage correct for 9 assignments}
\end{subfigure}
\end{tabular}
\caption{Visualizations of the maximum likelihood values when finding distributions using ordered probit. The left column shows a histogram of students who had a maximum likelihood value (bucketed to 5\%), for both students we could classify correctly and those we couldn't. The right column shows the ratio of the maximum likelihood value to the percentage of students with that value that we could guess correctly. The dotted line show the average, which we want to be close to 1.}
\end{figure*}

It is also interesting to try to get a feel for how these distributions actually looked. To avoid redundancy, from this point forward we will only consider the ordered probit model. As we have seen, the ordered logit model produces nearly identical results.

To start, let's consider a few test points, and look at their distributions after the first quiz (these are not real students). Consider three students taking 6.006 in the spring. Ben is a senior in course 6. He scored exactly average on the first two problem sets and first quiz. Jeremy is a freshman in course 6, and also scored average on the first two problem sets and first quiz. Paul is also a freshman in course 6. He scored average on the first two problem sets, but 10\% above average on the first quiz.

Figure 9 shows the distribution returned by ordered probit. We can see that both Ben and Jeremy are most likely to get a $B$, which is not surprising based on the fact that they scored average on everything. It may be surprising how much less likely Ben is to receive an $A$ then Jeremy, since the only difference between them is there age. However, as we have noted, in the training data, we see that $66.2\%$ of freshman received $A$'s in the class, while only $32.5\%$ of seniors did. With this in mind, their differences are not too surprising.

Let us take a more general look at the distributions in the test set. We attempt to represent this information graphically in Figure 10. Recall that for each student, we found the maximum likelihood estimate, and then classified their grade according to that.

Focus on Figure 10(a). Consider all the students who we were able to correctly guess their final grade. Let us focus on the values of the probabilities we assigned to their maximum likelihood estimates. The blue line represents a histogram of these values, bucketed to intervals of $5\%$. The green line is a similar histogram for students that we were not able to correctly guess their final grade. Figures 10(a), (c), and (e) represent these graphs after 3, 6, and 9 assignments, respectively. 

%The charts in the left column were all generated using distributions from the ordered logit model, and the charts in the right column were all generated using distributions from the ordered probit model. The rows correspond to different points throughout the semester. The first row is after three assignments (two pests and a quiz), the second row is after six assignments (four pests and two quizzes), and the last row is after all nine assignments.


%, we display the proportion of students who had an maximum likelihood of at least a certain value, for students classified correctly and incorrectly, for both the ordered logit and probit models. The different subfigures represent different points throughout the semester. 


There are a few interesting trends we can see from these graphs. First, we look at how the values of the maximum likelihoods differed for those students that we were able to classify correctly, and those who we classified incorrectly. We can see that we consistently tended to place higher probabilities on the students that we were ultimately able to classify correctly. This is comforting. It means that the models, in general, claimed to be less sure about the ones which they got wrong than the ones they got right.

%We're also interested in how the maximum likelihoods differed between the ordered logit and probit models. This actually is not especially apparent in these graphs, although it becomes much more obvious in the cumulative versions of these graphs (which we do not display here). In general, the ordered logit models tends to be more sure than the ordered probit model (corresponding to higher values for the maximum likelihood).

We can also see how the maximum likelihoods change throughout the semester. The general trend we see is that as time passes, the models assign higher probabilities to the ones they ultimately classified as correct, and lower probabilities to the ones they ultimately classified as incorrect. This is expected behavior. As the model receives more information, it is given more reason to be sure of the answers that it has classified correctly, and more reason to be unsure of the ones classified incorrectly.

Next let us focus on Figure10(b). Consider a group of students who we assigned maximum likelihood around $70\%$. We would expect to classify about $70\%$ of them correctly, and $30\%$ of them incorrectly. If we classified more than $70\%$ of them correctly, then we might suspect that we were being too modest, and we should have reported being more than $70\%$ confident about them.

Let $N_{0.7}^C$ represent the number of students which we classified correctly who had a maximum likelihood around $70\%$ (specifically, we mean $\pm 2.5\%$). Similarly, define $N_{0.7}^I$ for students who were classified incorrectly. We would hope it be the case that
\[\frac{N_{0.7}^C}{N_{0.7}^C + N_{0.7}^I} \approx 0.7.\]
In fact, we would want this to be the case for all values of the maximum likelihood $M$. To check this, we plotted
\[\frac{N_{M}^C}{N_{M}^C + N_{M}^I} \cdot \frac{1}{M},\]
hoping that these values would always be around 1. The results after 3 assignments are shown in Figure 10(b) (and after 6 and 9 assignments in Figures 10(d) and (f)). The dotted line shows the average (weighted by the number of points in that range) value.

It appears that after only 3 assignments, our model was slightly too confident in predicting its values, while after 9 assignments it was slightly too modest. However, none of the averages are particularly far from 1, which gives us an indication that our distributions are a good fit for our data.

Next, we consider a method for solving the grade distribution problem using results from regression.


\section{Fixed Cutoff Gaussian Probability}
\vspace*{-0.4em}
\section{Estimation}

%Predicting final score and letter grade are both nice, but neither is difficult to get a reasonable handle on. As has been previously discussed, past scores are an excellent predictor of future scores, with the personal statistics only adding minor variations. And classification also is easy to get approximately right--just take the final score prediction, and sort it by percentile into an A bin, a B bin, or a C bin, also as previously described. \\

%Because both of these problems are so easy to roughly solve, finding accurate solutions to them is less impressive than it otherwise might be. Moreover, it isn't exactly what students care most about. If a 6.006 student had done six percentage points above average on the first midterm, she would have no trouble deducing that her expected overall score in the class would be about six percent above the average, or that the likeliest grade for her to get in the class was an A. But neither is exactly the thing she is likely to be wondering; most likely, if she is asking herself a question related to what her grade in 6.006 is going to be, she is wondering what the \emph{odds} are that she'll get an A. 60\%? 80\%? 95\%? Naturally, the answer for this specific student depends in large part on her past academic record, since that determines her prior for what grade she'll get in 6.006. But assuming that her past academic record roughly matches the grade distribution of 6.006 (that is, in her past classes, her grades were split roughly 40/40/20) then it's quite possible for us to answer her question--and if her background is different from the grade distribution of 6.006, it's easy for her to generate the actual probabilities that apply to her, simply by multiplying ther true prior by the posterior we generate for her. \\

%It was, in fact, this question that motivated us to pursue this topic in the first place. How might we answer it? By making a few assumptions, it's straightforward for us to adapt the methods described previously to the new problem. \\

While the ordered logit/probit methods worked nicely, we really could not say much about their performance without having another method to compare them to. As such, we developed a method which we call Fixed-Cutoff Gaussian Probability Estimation (FCGPE). It relies heavily on the linear regression methods discussed earlier in the paper. Developed as a benchmark for ordered logit/probit, it turned out to be an interesting algorithm in its own right.

First, we take our numerical predictions obtained from ridge regression, and we assume that they are noisy. In particular, we assume that the true final score students will receive is normally distributed around the prediction we made. This is equivalent to the assumption that we made when performing ordered probit.

Here, instead of setting the variance of this Gaussian to a fixed 1 and scaling other parameters accordingly, we estimate $\sigma$. To solve this estimation problem, we first need to assume that the variance of the Gaussian noise is independent of what our prediction for the score is (note that we implicitly made this assumption in ordered probit as well).

Now, we can produce an estimate $\hat{\sigma}$ of the variance of the distribution in terms of our predictions $\hat{y}^{(i)}$ and the true values $y^{(i)}$ in the training and validation sets by using the formula
%
$$\hat{\sigma}^2 = \frac{1}{n} \cdot \sum_i^n (\hat{y}^{(i)} - y^{(i)})^2$$
%
Put more simply, we are taking the observed variance of our prediction's error in the training and validation sets, and using that observed variance as our estimated variance for the Gaussian noise around our predictions.

Recall how we solved the letter prediction problem in the regression section. We sorted all of the students by their predicted grades, and assigned final scores according to the distribution of final scores in the training and validation sets (also recall that this distribution was $41.0\%$ $A$'s, $41.7\%$ $B$'s, and $17.3\%$ $C$'s).

Using this same idea, we fix the cutoff for an $A$, $\mu_A$, at exactly the top $41.0^{th}$ percentile. Similarly, the cutoff for a $B$, $\mu_B$, at the bottom $17.3^{rd}$ percentile. With these fixed cutoffs for $A$'a, $B$'s, and $C$'s, we can now use the same probability distributions we used in the ordered probit section. Namely,
%
%Now that we have a probability distribution over the possible numerical grades that our hypothetical student will get, we can turn these probabilities over letter grades via the cutoffs we used for letter-grade prediction in the linear regression section. In other words, assuming our B/C cutoff (the $20^{\textrm{th}}$-percentile score) is $\mu_B$ and our A/B cutoff (the $60^{\textrm{th}}$-percentile score) is $\mu_A$, we estimate that the probability that the aforementioned student gets an A, a B, or a C is given by the following formulas:
\begin{equation*}
\begin{split} P(y = A) &= \Phi \left (\frac{\hat{y}-\mu_A}{\hat{\sigma}} \right )\\
P(y = B) &= \Phi \left (\frac{\mu_A-\hat{y}}{\hat{\sigma}} \right ) - \Phi \left (\frac{\mu_B-\hat{y}}{\hat{\sigma}} \right)\\
P(y = C) &= \Phi \left (\frac{\mu_B-\hat{y}}{\hat{\sigma}} \right).
\end{split}
\end{equation*}

\subsection{Results}

Fixed-Cutoff Gaussian Probability Estimation turned out to be a very successful method for calculating probability distributions over letter grades. It did well enough to outperform the more established ordered logit and probit methods, with a few caveats.

First, to get a feel for how FCGPE compared to ordered probit, recall the students shown in Figure 9, who took 6.006 during a spring semester. Ben is a senior in course 6, who has scored average on all assignments so far. Jeremy is a freshmen in course 6, also scoring average on all assignments. Paul is a freshman in course 6, who scored average on the first two problem sets and $10\%$ above average on the first quiz. 

Figure 11 shows the distributions for the same students, calculated using FCGPE. We see that it follows the same general trends as ordered probit. We see that Ben is less likely to receive an $A$ than Jeremy, despite having all the same scores so far in the class. Paul, having scored above average on the quiz, is more likely to receive a higher grade. We can see that FCGPE tends to make slightly bolder predictions than ordered probit.

\begin{figure}
\centering
\begin{tabular}{|c | c | c | c |}
\hline
Student & $P(y = A)$ & $P(y = B)$ & $P(y = C)$\\
\hline
Ben & $18.1\%$ & $72.0\%$ & $9.9\%$\\
\hline
Jeremy & $35.5\%$ & $61.1\%$ & $3.4\%$\\
\hline
Paul & $68.3\%$ & $31.2\%$ & $0.5\%$\\
\hline
\end{tabular}
\caption{The probability distributions predicted by ordered probit for students described in \textit{Understanding the Distributions} section.}
\end{figure}


\begin{figure}[t!]
\centering
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/fcgpe.png}
	\caption{Number of errors v. number of assignments}
\end{subfigure}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/fcgpe_percentage.png}
	\caption{Number of errors v. percentage of final grade}
\end{subfigure}
\caption{Depicts the value of the minimum negative log likelihood as we increase the number of assignments that we include as features. Each graph shows the results using FCGEP and probit (replicated from Figure 10 for comparison).}
\end{figure}

To see how they compared in a more general sense, see Figure 12. Here, as in Figure 8, we plot the negative log likelihood as a function of both number of assignments (Figure 12(a)) and percentage of final grade (Figure 12(b)). For comparison, we include the ordered probit curve from Figure 8 in these graphs as well. 

%For a detailed comparison of FCGPE with Ordered Probit, see Figure xx. % TODO figure name

Note that FCPGE has a consistently lower negative log likelihood, suggesting that it is making the more accurate predictions, until we reach the final, at which point FCPGE's negative log likelihood suddenly spikes to infinity. 

This is because unlike ordered probit, FCGPE uses fixed cutoffs. In other words, FCPGE assumes that there is a $100\%$ chance that the cutoffs will be exactly where they were in the average of the previous semesters. When there are few enough assignments included, this plainly false assumption doesn't cause any problems, because there's enough uncertainty in the final grade to give the predictions near the grade cutoffs sufficient caution. 

However, when all nine assignments are included, FCGPE suddenly makes predictions with extremely high confidence, and the false assumption that the cutoffs are precisely known dominates considerations. A single highly overconfident prediction, made on the assumption that the cutoff will be where it was last year, is enough to make the negative log likelihood reach extreme heights.

To solve this problem, the next step would likely be to get many samples of where the cutoffs were in past semesters, to get a sense of what the probability distribution over the cutoff locations might look like. However, to do this accurately, we would need access to substantially more semester data than we had.

\begin{figure*}
\centering
\begin{tabular}{c c}
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/adam3dist.png}
	\caption{Histogram of ML's for 3 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1adam3.png}
	\caption{Ratio of ML to percentage correct for 3 assignments}
\end{subfigure}
\\
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/adam6dist.png}
	\caption{Histogram of ML's for 6 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1adam6.png}
	\caption{Ratio of ML to percentage correct for 6 assignments}
\end{subfigure}
\\
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/adam9dist.png}
	\caption{Histogram of ML's for 9 assignments}
\end{subfigure}
&
\begin{subfigure}[b]{3in}
	\includegraphics[width=3in]{plots/1adam9.png}
	\caption{Ratio of ML to percentage correct for 9 assignments}
\end{subfigure}
\end{tabular}
\caption{Visualizations of the maximum likelihood values when finding distributions using FCGPE. The left column shows a histogram of students who had a maximum likelihood value (bucketed to 5\%), for both students we could classify correctly and those we couldn't. The right column shows the ratio of the maximum likelihood value to the percentage of students with that value that we could guess correctly. The dotted line show the average, which we want to be close to 1.}
\end{figure*}

FCGPE's edge over ordered probit appears to mostly manifest itself in more aggressive predictions, which are on average warranted (until nine assignments, when they were not). Figure 13 replicates Figure 10 using FCGPE instead of ordered probit. Looking at the graphs in the left column, we can see the FCGPE tends to make more confident predictions than ordered probit, especially after all nine assignments are included.

From the column on the right, we see that FCGPE does a better job of keeping the ratio of maximum likelihood to percentage correct closer to 1, as we discussed is desirable. This tells us that FCGPE is using roughly the right amount of confidence, with the exception of overconfidence at the end. 


%See Figure xx % TODO figure name
%for a comparison of the aggressiveness of predictions made by Ordered Probit and by FCGPE, compared to how often those predictions should have been right. From the figure, it is apparent that FCPGE (before it collapses) makes predictions whose probabilities are more well-matched to the actual rate of occurrence of the predicted event than Ordered Probit (in other words, the things that FCPGE predicted would happen 80\% of the time tended to happen about 80\% of the time, whereas the things that Ordered Probit predicted would happen 80\% of the time tended to happen a little more often than that). 

We also tried FCPGE, but with the assumption that true final score would be logistically, not normally, distributed about the predicted final scores, as is assumed with ordered logit.  We found that this led to predictions that were too timid, and a negative log likelihood that was substantially less impressive than either the original version of FCPGE, or either of ordered logit or probit. 


\section{Discussion}

We began by solving the simple numerical prediction problem. However, realizing that it is not numerical scores that interest students, we moved on to solving the letter prediction problem. But predicting the final scores of students still seemed to be lacking. Certainly a student not only wants to know what the TA's think their final grade will be, but how likely it is. Thus, we also focused on the letter distribution problem.

Solving numerical prediction involved feeding our data into ridge regression, for which we know the closed form solution. We then developed a scheme for solving the letter prediction problem using our solutions to ridge regression. This method turned out to perform just as well as any other methods we tried. Finally, we developed a method for solving the letter distribution problem using the results of ridge regression, which we named FCPGE. For the most part, this method performed very well. However, the nature of this solution lead to overconfidence, which ultimately lead to an incorrect assignment of zero probability to an event.

To solve the letter prediction and distribution problems, we also implemented ordered logit and probit, which are more obvious models for this problem. These two models produced essentially identical results. Interestingly, they did not seem to perform any better than our makeshift methods based off of regression. However, when solving the letter prediction problem, these models did have the benefit that they never assigned zero probability to any event, and thus never suffered the infinite loss incurred by FCPGE.

In the end, we have a few general lessons learned from all of our experimentation. First, we know that we can, in fact, make fairly accurate predictions about student grades from an early point in the semester. Even after only quiz 1, we can predict final grades with $\sim65\%$ accuracy. After quiz 2, we have $\sim80\%$ accuracy.

It is interesting that the 6.006 policy is to traditionally keep the grading process very opaque as to not make any false promises, when it seems that we could offer the students much more useful advice as to their standing in the class.

We also learned that all of our more complicated models were not able to really perform any better than simple linear regression. We think this reflects the unpredictability of both the students and the professors. Some students are always going to perform very far from our early expectation, and we will not be able to accurately predict their final scores. Similarly, some professor will choose thresholds for the class that differ significantly from other semesters, and we cannot model this behavior.

A surprising result is that problem sets seem to matter proportionally as much as the quizzes, even early on in the semester. After the first quiz, 1/3 of the students' overall problem set grade is determined, but only 2/7 of the overall test grade is determined. As a result we speculated that quiz 1 would matter more than the problem sets proportionally, although we did not see this trend. In fact, we sometimes saw the problem sets mattered even more than quizzes in early predictions.

Our experimentation did not result in a model emerging as the obvious best model for this problem. However, we saw multiple models converge to almost the exact same results, suggesting that we were approaching the best error rates we could hope to achieve.

\end{document}